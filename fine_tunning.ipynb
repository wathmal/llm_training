{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Deps"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d710c04f635b847b"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.15.2+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:30.627788600Z",
     "start_time": "2023-10-02T12:04:24.010282500Z"
    }
   },
   "id": "2c642841f7cbfe92"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (4.27.2)\n",
      "Requirement already satisfied: torchdata in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: loralib in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: peft in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torchdata) (1.26.16)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torchdata) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchdata) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchdata) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from torch==2.0.1->torchdata) (3.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from rouge_score) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from peft) (0.23.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from peft) (0.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.1->torchdata) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wathmal\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torchdata datasets evaluate rouge_score loralib peft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:37.590092700Z",
     "start_time": "2023-10-02T12:04:30.629788400Z"
    }
   },
   "id": "641cbcad1b17a4cb"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:37.609223300Z",
     "start_time": "2023-10-02T12:04:37.592087700Z"
    }
   },
   "id": "f11070547eab1ea9"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:41.057019200Z",
     "start_time": "2023-10-02T12:04:37.606226700Z"
    }
   },
   "id": "9d59ef847e4c1a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:45.903969200Z",
     "start_time": "2023-10-02T12:04:41.053019500Z"
    }
   },
   "id": "eeeba7a681bfe33d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's explore the model parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a3b5564ce91050"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:45.919990800Z",
     "start_time": "2023-10-02T12:04:45.906969300Z"
    }
   },
   "id": "b68b3ad0729a2ee5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5eb440d9af99c46"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: What are you working on?\n",
      "#Person2#: I'm figuring out my budget.\n",
      "#Person1#: What budget?\n",
      "#Person2#: I'm making a shopping budget, so that I don't spend too much money.\n",
      "#Person1#: How much money can you spend?\n",
      "#Person2#: I can only spend $ 300 a month.\n",
      "#Person1#: Why only $ 300?\n",
      "#Person2#: I need to save the rest.\n",
      "#Person1#: For what?\n",
      "#Person2#: I need to pay my bills.\n",
      "#Person1#: Your budget is a good idea.\n",
      "#Person2#: I know. It's going to save me a lot of money, I hope.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# is making a shopping budget to save the rest of the money. #Person1# thinks it's a good idea.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "How much money can you spend?\n"
     ]
    }
   ],
   "source": [
    "index = 222\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:50.712631200Z",
     "start_time": "2023-10-02T12:04:45.921990600Z"
    }
   },
   "id": "9596b15942e863b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to Fine tune, first create the prompt-response pairs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9c184534b09264b"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [12198, 1635, 1737, 8, 826, 3634, 5, 1713, 345, 13515, 536, 4663, 10, 363, 103, 25, 241, 12, 214, 81, 140, 58, 1713, 345, 13515, 357, 4663, 10, 571, 81, 39, 2705, 3187, 44, 1900, 58, 1713, 345, 13515, 536, 4663, 10, 37, 1348, 2769, 13, 66, 82, 2996, 19, 756, 11989, 5, 1713, 345, 13515, 357, 4663, 10, 86, 84, 1426, 410, 25, 129, 8, 2030, 6784, 58, 1713, 345, 13515, 536, 4663, 10, 86, 17082, 27, 530, 3, 9, 3, 3916, 5, 1713, 345, 13515, 357, 4663, 10, 2114, 25, 1204, 136, 19397, 58, 1713, 345, 13515, 536, 4663, 10, 2163, 6, 27, 43, 6, 11, 386, 648, 16, 792, 5, 1713, 345, 13515, 357, 4663, 10, 2114, 25, 118, 3, 9, 853, 2488, 58, 1713, 345, 13515, 536, 4663, 10, 27, 43, 118, 3, 9, 853, 26864, 7, 1208, 16, 1567, 13, 2116, 21, 192, 203, 5, 1713, 345, 13515, 357, 4663, 10, 3963, 25, 1715, 16, 136, 1886, 1087, 58, 1713, 345, 13515, 536, 4663, 10, 27, 47, 46, 30277, 7, 372, 1144, 16, 1900, 5, 1713, 345, 13515, 357, 4663, 10, 363, 2600, 33, 25, 207, 44, 58, 1713, 345, 13515, 536, 4663, 10, 27, 183, 207, 44, 21710, 11, 953, 9999, 5, 1713, 345, 13515, 357, 4663, 10, 148, 33, 1287, 5, 20698, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1713, 345, 13515, 357, 4663, 987, 7, 1713, 345, 13515, 536, 4663, 633, 746, 6, 114, 2705, 3187, 6, 8, 2030, 6784, 6, 19397, 6, 1886, 1087, 6, 11, 6847, 2100, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    # JS: const prompt = dialogueList.map(dialogue => startPrompt + dialogue + endPrompt);\n",
    "    example['prompt'] = prompt\n",
    "    # Why padding is Max Length?\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Element's 'input_ids' and 'label' keys have the tokenised prompt and response.\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# print(tokenized_datasets['train'][200])\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary', 'prompt'])\n",
    "print(tokenized_datasets['train'][200])\n",
    "\n",
    "# \n",
    "# print(tokenized_datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:50.755153100Z",
     "start_time": "2023-10-02T12:04:50.711628900Z"
    }
   },
   "id": "ad9c2c4f6f168ee7"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "output_dir = './full-dialogue-summary-training'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=10,  # Increase the number of epochs for more training.\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,   # Reduce the frequency of logging for better readability.\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:50.769349Z",
     "start_time": "2023-10-02T12:04:50.756153200Z"
    }
   },
   "id": "ed2c8d9084394509"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # decode preds and labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:51.867930600Z",
     "start_time": "2023-10-02T12:04:50.772863900Z"
    }
   },
   "id": "8a369c832853ba1a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "# tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:51.884037200Z",
     "start_time": "2023-10-02T12:04:51.868933200Z"
    }
   },
   "id": "2187432e7f3a2724"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T12:04:55.206434800Z",
     "start_time": "2023-10-02T12:04:51.885037200Z"
    }
   },
   "id": "fc92255f894fd78"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='15580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/15580 : < :, Epoch 0.00/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=15580, training_loss=42.07952503209243, metrics={'train_runtime': 23397.0484, 'train_samples_per_second': 5.325, 'train_steps_per_second': 0.666, 'total_flos': 2.31619568861184e+16, 'train_loss': 42.07952503209243, 'epoch': 10.0})"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T18:43:23.379714300Z",
     "start_time": "2023-10-02T12:13:26.154460100Z"
    }
   },
   "id": "a048410e1f9745da"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Do you have a CD-ROM drive?\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FINE TUNED:\n",
      "You'd like to add a CD-ROM drive to your software.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./full-dialogue-summary-training/checkpoint-15500\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)\n",
    "original_model.to(device)\n",
    "instruct_model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)).to(device)\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{original_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - FINE TUNED:\\n{instruct_model_text_output}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T21:39:17.711010200Z",
     "start_time": "2023-10-02T21:39:00.020232800Z"
    }
   },
   "id": "5104b4dcc01bbd34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "91e8826faab9021b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "llmtraining",
   "language": "python",
   "display_name": "llmtraining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
